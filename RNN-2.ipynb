{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings1 = pd.read_csv(\"tweets_50k_all.csv\", header = None, index_col = False)\n",
    "data_embeddings1.columns = [\"index\",\"text\"]\n",
    "\n",
    "data_embeddings2 = pd.read_csv(\"tweets_70k_all.csv\", header = None, index_col = False)\n",
    "data_embeddings2.columns = [\"index\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = pd.concat([data_embeddings1,data_embeddings2.iloc[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets_50k_labeled.csv\", header = None, index_col = False)\n",
    "data.columns = [\"text\",\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning1(tweet):\n",
    "    return ' '.join(word for word in tweet.split(' ') if not word.startswith('#') and not word.startswith('@') and not \"&gt\" in word and not 'http' in word and not word.startswith('rt') and not \"&amp\" in word)\n",
    "\n",
    "def cleaning2(tweet):\n",
    "    return tweet.replace(\",\", \" \").replace(\"...\",\" \").replace(\"\\\"\",\" \").replace(\"/\",\" \").replace(\".\",\" \").replace(\":\",\" \").replace(\"!\",\" \").replace(\"?\",\" \").replace(\";\",\" \").replace(\"-\",\" \").replace(\"\\r\\n\",\" \").replace(\"  \",\" \")\n",
    "\n",
    "def cleaning3(tweet):\n",
    "    return tweet.replace(\"č\",\"c\").replace(\"ć\",\"c\").replace(\"š\",\"s\").replace(\"đ\",\"dj\").replace(\"ž\",\"z\")\n",
    "\n",
    "def cleaning4(tweet):\n",
    "    return tweet.replace(\"а\",\"a\").replace(\"б\",\"b\").replace(\"в\",\"v\").replace(\"г\",\"g\").replace(\"д\",\"d\").replace(\"ђ\",\"dj\").replace(\"е\",\"e\").replace(\"ж\",\"z\").replace(\"з\",\"z\").replace(\"и\",\"i\").replace(\"ј\",\"j\").replace(\"к\",\"k\").replace(\"л\",\"l\").replace(\"љ\",\"lj\").replace(\"м\",\"m\").replace(\"н\",\"n\").replace(\"њ\",\"nj\").replace(\"о\",\"o\").replace(\"п\",\"p\").replace(\"р\",\"r\").replace(\"с\",\"s\").replace(\"т\",\"t\").replace(\"ћ\",\"c\").replace(\"у\",\"u\").replace(\"ф\",\"f\").replace(\"х\",\"h\").replace(\"ц\",\"c\").replace(\"ч\",\"c\").replace(\"џ\",\"dz\").replace(\"ш\",\"s\")\n",
    "\n",
    "def cleaning5(tweet):\n",
    "    if \"ā\" in tweet or \"y\" in tweet or \"ç\" in tweet or \"ы\" in tweet or \"й\" in tweet or \"ę\" in tweet or \"ż\" in tweet or \"ý\" in tweet or \"ě\" in tweet or \"á\" in tweet or \"щ\" in tweet or \"ь\" in tweet or \"ą\" in tweet or \"ю\" in tweet or \"w\" in tweet or \"ø\" in tweet or \"æ\" in tweet or \"å\" in tweet or \"я\" in tweet:\n",
    "        return(\"\")\n",
    "    else:\n",
    "        return(tweet)\n",
    "    \n",
    "def tweet_cleaning(tweet):\n",
    "    return cleaning5(cleaning4(cleaning3(cleaning2(cleaning1(tweet)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = [tweet_cleaning(x.lower()) for x in np.array(data.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings.text = [tweet_cleaning(x.lower()) for x in np.array(data_embeddings.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Word2vec model (CBOW based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for tweet in data_embeddings['text']:\n",
    "    sentences.append(tweet.split(sep = \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w2v_cbow = Word2Vec(\n",
    "        sentences,\n",
    "        size=300,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('predsednik', 0.5322154760360718),\n",
       " ('on', 0.4874170422554016),\n",
       " ('veber', 0.4455837309360504),\n",
       " ('srbe', 0.4424019157886505),\n",
       " ('si…', 0.4303600490093231),\n",
       " ('omalovazavati', 0.4233503043651581),\n",
       " ('resenje', 0.41691774129867554),\n",
       " ('ribi', 0.4141949713230133),\n",
       " ('neko', 0.4055980145931244),\n",
       " ('mu', 0.39992427825927734)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_cbow.most_similar(\"vucic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('vuk', 0.5657670497894287),\n",
       " ('jankovic', 0.5298805832862854),\n",
       " ('tadic', 0.46134862303733826),\n",
       " ('lesinari', 0.44987910985946655),\n",
       " ('doslovno', 0.44896531105041504),\n",
       " ('uteraju', 0.44756633043289185),\n",
       " ('rodoljubivi', 0.4253556430339813),\n",
       " ('trikovima', 0.42190003395080566),\n",
       " ('bosko', 0.4213244915008545),\n",
       " ('haradinaj', 0.4182480573654175)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_cbow.most_similar(\"djilas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sad', 0.00019485086),\n",
       " ('onda', 0.00012481447),\n",
       " ('medija', 0.00011449565),\n",
       " ('mozda', 8.968371e-05),\n",
       " ('dalje', 8.467133e-05),\n",
       " ('oruzjem', 8.122055e-05),\n",
       " ('zamrzavanjem', 7.9763886e-05),\n",
       " ('to', 7.756317e-05),\n",
       " ('zenu', 7.2660136e-05),\n",
       " ('pricao', 6.8405534e-05)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_cbow.predict_output_word(\"granice ce biti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('kim', 0.586980938911438),\n",
       " ('presevo', 0.5148137211799622),\n",
       " ('kosova', 0.48257994651794434),\n",
       " ('lideralne', 0.4640258848667145),\n",
       " ('on', 0.46144890785217285),\n",
       " ('resenje', 0.4501580595970154),\n",
       " ('izjavom', 0.4423031806945801),\n",
       " ('tvorevina', 0.4342890679836273),\n",
       " ('moramo', 0.4251520037651062),\n",
       " ('predao', 0.4216660261154175)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_cbow.most_similar(\"kosovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124022"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_cbow.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Word2vec model - Skipgram based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_w2v_skipgram = Word2Vec(\n",
    "        sentences,\n",
    "        size=300,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=10,\n",
    "        sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('aleksandar', 0.451048880815506),\n",
       " ('pozeleo', 0.4146824777126312),\n",
       " ('odmogne', 0.4079938232898712),\n",
       " ('kukavica', 0.3963949978351593),\n",
       " ('@markodjuric', 0.3907366394996643),\n",
       " ('imao', 0.39064130187034607),\n",
       " ('istrajno', 0.39010918140411377),\n",
       " ('iznese', 0.3899855613708496),\n",
       " ('selakovic', 0.3899197280406952),\n",
       " ('spava', 0.3898516893386841)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_skipgram.most_similar(\"vucic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prevarant', 0.5462846159934998),\n",
       " ('pacoli', 0.5268086791038513),\n",
       " ('muljator', 0.5240795016288757),\n",
       " ('jeremic', 0.5074059963226318),\n",
       " ('unustili', 0.5064142942428589),\n",
       " ('svestenika', 0.5063611268997192),\n",
       " ('ljoticevac', 0.49912264943122864),\n",
       " ('ljimaj', 0.4910419285297394),\n",
       " ('tajkun', 0.49070703983306885),\n",
       " ('fra', 0.48984336853027344)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v_skipgram.most_similar(\"djilas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mradosavljevic\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = tweet_w2v_cbow.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = vocab_size\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "X = tokenizer.texts_to_sequences(data['text'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "Y = pd.get_dummies(data['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1281, 59) (1281, 2)\n",
      "(631, 59) (631, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size,output_dim= embedding_size,input_length = X.shape[1], weights=[pretrained_weights]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(5, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 9s - loss: 0.7013 - acc: 0.5180\n",
      "Epoch 2/20\n",
      " - 9s - loss: 0.6698 - acc: 0.5843\n",
      "Epoch 3/20\n",
      " - 9s - loss: 0.6358 - acc: 0.6124\n",
      "Epoch 4/20\n",
      " - 8s - loss: 0.6033 - acc: 0.6959\n",
      "Epoch 5/20\n",
      " - 8s - loss: 0.5614 - acc: 0.7342\n",
      "Epoch 6/20\n",
      " - 8s - loss: 0.5052 - acc: 0.7639\n",
      "Epoch 7/20\n",
      " - 9s - loss: 0.4499 - acc: 0.8146\n",
      "Epoch 8/20\n",
      " - 8s - loss: 0.3988 - acc: 0.8404\n",
      "Epoch 9/20\n",
      " - 8s - loss: 0.3586 - acc: 0.8630\n",
      "Epoch 10/20\n",
      " - 8s - loss: 0.2905 - acc: 0.8872\n",
      "Epoch 11/20\n",
      " - 8s - loss: 0.2495 - acc: 0.9145\n",
      "Epoch 12/20\n",
      " - 8s - loss: 0.2185 - acc: 0.9270\n",
      "Epoch 13/20\n",
      " - 8s - loss: 0.1556 - acc: 0.9582\n",
      "Epoch 14/20\n",
      " - 8s - loss: 0.1375 - acc: 0.9528\n",
      "Epoch 15/20\n",
      " - 8s - loss: 0.1203 - acc: 0.9684\n",
      "Epoch 16/20\n",
      " - 8s - loss: 0.0898 - acc: 0.9762\n",
      "Epoch 17/20\n",
      " - 8s - loss: 0.0755 - acc: 0.9840\n",
      "Epoch 18/20\n",
      " - 9s - loss: 0.0705 - acc: 0.9848\n",
      "Epoch 19/20\n",
      " - 8s - loss: 0.0635 - acc: 0.9856\n",
      "Epoch 20/20\n",
      " - 8s - loss: 0.0518 - acc: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29005f4f358>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 20, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet=np.array(['Zemlja napreduje uz naseg predsednika','Ova naprednjacka vlast je katastrofa','Zuti lopovi opet hoce vlast'])\n",
    "new_tweet=(tokenizer.texts_to_sequences(new_tweet))\n",
    "new_tweet=pad_sequences(new_tweet,maxlen=X.shape[1])\n",
    "new_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural network using mean summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_mean_summarized = pd.DataFrame()\n",
    "for sentence in X:\n",
    "    X1_mean_summarized = X1_mean_summarized.append(pd.DataFrame(pretrained_weights[sentence[sentence > 0]].mean(axis = 0)).transpose())\n",
    "X_mean_summarized = X1_mean_summarized.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean_summarized_train, X_mean_summarized_test, Y_mean_summarized_train, Y_mean_summarized_test = train_test_split(X_mean_summarized,Y[X1_mean_summarized.max(axis=1)>0], test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20,activation = \"relu\", input_dim = 300))\n",
    "model.add(Dense(2,activation = \"softmax\"))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_mean_summarized_train, Y_mean_summarized_train, epochs = 100, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(X_mean_summarized_test, Y_mean_summarized_test, verbose = 2, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural network using max summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_max_summarized = pd.DataFrame()\n",
    "for sentence in X[X1_mean_summarized.max(axis=1)>0]:\n",
    "    X1_max_summarized = X1_max_summarized.append(pd.DataFrame(pretrained_weights[sentence[sentence > 0]].max(axis = 0)).transpose())\n",
    "X_max_summarized = X1_max_summarized.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_max_summarized_train, X_max_summarized_test, Y_max_summarized_train, Y_max_summarized_test = train_test_split(X_max_summarized,Y[X1_mean_summarized.max(axis=1)>0], test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20,activation = \"relu\", input_dim = 300))\n",
    "model.add(Dense(2,activation = \"softmax\"))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_max_summarized_train, Y_max_summarized_train, epochs = 100, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(X_max_summarized_test, Y_max_summarized_test, verbose = 2, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
